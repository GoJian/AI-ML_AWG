{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:29:47.348202Z",
     "iopub.status.busy": "2025-05-09T16:29:47.347907Z",
     "iopub.status.idle": "2025-05-09T16:29:53.829011Z",
     "shell.execute_reply": "2025-05-09T16:29:53.828427Z",
     "shell.execute_reply.started": "2025-05-09T16:29:47.348179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/Viv')\n",
    "os.mkdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/CC2')\n",
    "os.mkdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/Chc20')\n",
    "os.mkdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/F')\n",
    "os.mkdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/GC')\n",
    "\n",
    "#file names\n",
    "primary_file = ['Viv','CC2','Chc20','F','GC']\n",
    "\n",
    "sub_files = {\n",
    "    'Viv':['LSDS-1_immunostaining_Viv20_Mouse_Eye.zip','LSDS-1_immunostaining_Viv19_Mouse_Eye.zip','LSDS-1_immunostaining_Viv18_Mouse_Eye.zip','LSDS-1_immunostaining_Viv17_Mouse_Eye.zip','LSDS-1_immunostaining_Viv16_Mouse_Eye.zip','LSDS-1_immunostaining_Viv15_Mouse_Eye.zip'],\n",
    "    'CC2':['LSDS-1_immunostaining_CC2_20_Mouse_Eye.zip','LSDS-1_immunostaining_CC2_18_Mouse_Eye.zip','LSDS-1_immunostaining_CC2_17_Mouse_Eye.zip','LSDS-1_immunostaining_CC2_16_Mouse_Eye.zip','LSDS-1_immunostaining_CC2_15_Mouse_Eye.zip'],\n",
    "    'Chc20':['LSDS-1_immunostaining_Chc20_Mouse_Eye.zip'],\n",
    "    'F':['LSDS-1_immunostaining_F20_Mouse_Eye.zip','LSDS-1_immunostaining_F19_Mouse_Eye.zip','LSDS-1_immunostaining_F18_Mouse_Eye.zip','LSDS-1_immunostaining_F17_Mouse_Eye.zip','LSDS-1_immunostaining_F16_Mouse_Eye.zip','LSDS-1_immunostaining_F15_Mouse_Eye.zip'],\n",
    "    'GC':['LSDS-1_immunostaining_GC20_Mouse_Eye.zip','LSDS-1_immunostaining_GC19_Mouse_Eye.zip','LSDS-1_immunostaining_GC18_Mouse_Eye.zip','LSDS-1_immunostaining_GC17_Mouse_Eye.zip','LSDS-1_immunostaining_GC16','LSDS-1_immunostaining_GC15_Mouse_Eye.zip'],\n",
    "}\n",
    "\n",
    "# !wget -O \"file.zip\" \"https://osdr.nasa.gov/geode-py/ws/studies/OSD-557/download?source=datamanager&file=LSDS-1_immunostaining_Viv20_Mouse_Eye.zip\"\n",
    "# !unzip \"file.zip\" -d /content/unzipped_folder\n",
    "\n",
    "#import the files and unzip zip files\n",
    "for i in range(len(primary_file)):\n",
    "    os.chdir(f'/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/{primary_file[i]}')\n",
    "    for j in range(len(sub_files[primary_file[i]])):\n",
    "        !wget -O \"{sub_files[primary_file[i]][j]}.zip\" \"https://osdr.nasa.gov/geode-py/ws/studies/OSD-557/download?source=datamanager&file={sub_files[primary_file[i]][j]}\"\n",
    "        !unzip \"{sub_files[primary_file[i]][j]}.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "primary_file = ['Viv','CC2','Chc20','F','GC']\n",
    "\n",
    "#remove zip files\n",
    "for i in range(len(primary_file)):\n",
    "    os.chdir(f'/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/{primary_file[i]}')\n",
    "    for j in range(len(sub_files[primary_file[i]])):\n",
    "        !rm -rf \"{sub_files[primary_file[i]][j]}.zip\"\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "os.mkdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/data2')\n",
    "# os.mkdir('data2/CH1')\n",
    "os.mkdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/data2/CH2')\n",
    "# os.mkdir('data2/CH3')\n",
    "# os.mkdir('data2/Overlay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move all the required images for training to data2/CH1 or data/CH2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "var = \"CH2\"\n",
    "subc = \"2\"\n",
    "destination_folder = '/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/data2/'+var\n",
    "shifted_files = os.listdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/data2/'+var)\n",
    "for i in range(len(primary_file)):\n",
    "    lsds_files = os.listdir(f'/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/{primary_file[i]}')\n",
    "    for j in range(len(lsds_files)):\n",
    "        lsds = lsds_files[j]\n",
    "        ch_files = os.listdir(f'/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/{primary_file[i]}/{lsds}')\n",
    "        for k in range(len(ch_files)):\n",
    "            ch = ch_files[k]\n",
    "            shifted_files = os.listdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/data2/'+var)\n",
    "            if primary_file[i] == 'GC' and lsds == 'GC20_Mouse_Eye':\n",
    "                subch_files = os.listdir(f'/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/{primary_file[i]}/{lsds}/{ch}')\n",
    "                #print(subch_files)\n",
    "                for subch in subch_files:\n",
    "                    if subch.endswith('.tif'):\n",
    "                        if subch[:-4][-1] == subc and subch not in shifted_files:\n",
    "                            shutil.move(f'/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/{primary_file[i]}/{lsds}/{ch}/{subch}', destination_folder)\n",
    "                        else:\n",
    "                            print(f\"{subch} file present\")\n",
    "            else:\n",
    "                if ch.endswith('.tif'):\n",
    "                    if ch[:-4][-1] == subc and ch not in shifted_files:\n",
    "                        shutil.move(f'/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/{primary_file[i]}/{lsds}/{ch}', destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "l = os.listdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/data2/CH2')\n",
    "for i in l:\n",
    "    if i.split('.')[0][-1]!='2':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd '/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/'\n",
    "#remove group folders\n",
    "!rm -rf \"CC2\"\n",
    "!rm -rf \"Chc20\"\n",
    "!rm -rf \"F\"\n",
    "!rm -rf \"GC\"\n",
    "!rm -rf \"Viv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#check if group folders removed and only data2/ folder\n",
    "os.listdir('/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def normalizeNumpyImage(image):\n",
    "    \"\"\"\n",
    "    Normalize a NumPy image to [0, 1]\n",
    "    \"\"\"\n",
    "    return (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-8)\n",
    "\n",
    "class CustomNormalizeTransform:\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img).astype(np.float32)\n",
    "        img_np = normalizeNumpyImage(img_np)\n",
    "        return transforms.ToTensor()(img_np)  # Converts to [C,H,W] tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Green channel\n",
    "# class ExtractGreenChannel:\n",
    "#     def __call__(self, img: Image.Image):\n",
    "#         img = TF.to_tensor(img)  # Convert to [3, H, W]\n",
    "#         green = img[1].unsqueeze(0)  # Extract green channel and keep it [1, H, W]\n",
    "#         return green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Only required for FFT\n",
    "# import torch.fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========== 1. Hyperparameters ==========\n",
    "image_size_width = 1440  # You can change this depending on your image resolution\n",
    "image_size_height = 1920\n",
    "batch_size = 4\n",
    "latent_dim = 1024\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== 2. Transform & Dataset ==========\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size_width, image_size_height)),\n",
    "    #ExtractGreenChannel(),\n",
    "    #transforms.Normalize(mean=[0.0], std=[1.0])\n",
    "    CustomNormalizeTransform(),\n",
    "    # transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(\"/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/data2\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "in_channels = 3 # single band or 3 band\n",
    "\n",
    "# ========== 3. Encoder-Decoder Model ==========\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1024):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 8, 4, stride=2, padding=1),     # [B, 8, 720, 960]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 4, stride=2, padding=1),    # [B, 16, 360, 480]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 4, stride=2, padding=1),   # [B, 32, 180, 240]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),   # [B, 64, 90, 120]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # [B, 128, 45, 60]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 45 * 60, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1024):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 45 * 60),\n",
    "            nn.Unflatten(1, (128, 45, 60)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # [B, 64, 90, 120]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),   # [B, 32, 180, 240]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),   # [B, 16, 360, 480]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, 4, stride=2, padding=1),    # [B, 8, 720, 960]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 3, 4, stride=2, padding=1),     # [B, 3, 1440, 1920]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        # ===================Tweaks for FFT===================\n",
    "        # self.image_encoder = Encoder(latent_dim // 2)\n",
    "        # self.fourier_encoder = Encoder(latent_dim // 2)\n",
    "        # self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "        # ===================Tweaks for FFT===================\n",
    "        # ft = torch.fft.fft2(x)\n",
    "        # ft_mag = torch.abs(ft)\n",
    "        # ft_mag = (ft_mag - ft_mag.mean()) / (ft_mag.std() + 1e-6) #normalization\n",
    "        # z_img = self.image_encoder(x)\n",
    "        # z_ft = self.fourier_encoder(ft_mag)\n",
    "        # x_recon = self.decoder(torch.cat([z_img, z_ft], dim=1))\n",
    "        # return x_recon\n",
    "\n",
    "# ========== 4. Training Loop ==========\n",
    "\n",
    "model = Autoencoder(latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, 1440, 1920))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# Define your custom weighted MSE loss\n",
    "def weighted_mse_loss(input, target, weight=None):\n",
    "    if weight is None:\n",
    "        weight = (target > 0.05).float() * 5 + 1  # higher weight to non-background\n",
    "    return ((input - target) ** 2 * weight).mean()\n",
    "\n",
    "# Initialize model, optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "save_path = \"/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/models/autoencoder_checkpoint.pth\"\n",
    "\n",
    "num_epochs = 100\n",
    "loss_history = []\n",
    "patience = 10  # stop if loss is constant for 10 epochs\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = weighted_mse_loss(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Weighted Loss: {epoch_loss:.6f}\")\n",
    "        loss_history.append(epoch_loss)\n",
    "\n",
    "        # Early stopping if loss doesn't change much for `patience` epochs\n",
    "        if len(loss_history) >= patience:\n",
    "            recent_losses = loss_history[-patience:]\n",
    "            if max(recent_losses) - min(recent_losses) < 1e-4:\n",
    "                print(f\"Loss unchanged for {patience} epochs. Early stopping.\")\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"Model saved to {save_path}\")\n",
    "                break\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if \"CUDA out of memory\" in str(e):\n",
    "        print(\"CUDA Out of Memory error encountered! Saving model.\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "    else:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#normal loss \n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0\n",
    "#     for batch in dataloader:\n",
    "#         imgs, _ = batch\n",
    "#         imgs = imgs.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(imgs)\n",
    "#         loss = criterion(outputs, imgs)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), \"/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/models/encoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/models/autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#fft\n",
    "# torch.save(model.image_encoder.state_dict(), \"image_encoder.pth\")\n",
    "# torch.save(model.fourier_encoder.state_dict(), \"fourier_encoder.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(latent_dim=512).to(device)\n",
    "encoder.load_state_dict(torch.load('encoder.pth', map_location=device))\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Autoencoder(latent_dim).to(device)\n",
    "model.load_state_dict(torch.load('autoencoder.pth', map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Assume `dataloader` provides your images and you've already defined your transform\n",
    "data_iter = iter(dataloader)\n",
    "images, _ = next(data_iter) \n",
    "images = images[:6].to(device)  # Take first 6 images from the batch\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(images)\n",
    "\n",
    "# Convert tensors to numpy images for plotting\n",
    "def to_numpy(img_tensor):\n",
    "    img = img_tensor.cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(to_numpy(outputs[i]))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Reconstructed Image {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Fetch a batch\n",
    "data_iter = iter(dataloader)\n",
    "images, _ = next(data_iter) \n",
    "images = images.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(images)\n",
    "\n",
    "# Helper function\n",
    "def to_numpy(img_tensor):\n",
    "    img = img_tensor.cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "# Plot only however many images are available\n",
    "num_images = images.shape[0]\n",
    "cols = min(num_images, 3)\n",
    "rows = (num_images + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "\n",
    "# Make sure axes is iterable\n",
    "axes = axes.flatten() if num_images > 1 else [axes]\n",
    "\n",
    "for i in range(num_images):\n",
    "    axes[i].imshow(to_numpy(outputs[i]))\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Reconstructed Image {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Original and Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(18, 6))\n",
    "\n",
    "for i in range(4):\n",
    "    # Original\n",
    "    axes[0, i].imshow(to_numpy(images[i]))\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 2:\n",
    "        axes[0, i].set_title(\"Originals\")\n",
    "\n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(to_numpy(outputs[i]))\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 2:\n",
    "        axes[1, i].set_title(\"Reconstructions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_numpy(img_tensor):\n",
    "    img = img_tensor.detach().cpu()\n",
    "    if img.ndim == 3:\n",
    "        img = img.permute(1, 2, 0)  # [C, H, W] -> [H, W, C]\n",
    "    return img.numpy()\n",
    "\n",
    "# === Error Map ===\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "for i in range(4):\n",
    "    # Ensure same shape\n",
    "    original = images[i]\n",
    "    recon = outputs[i]\n",
    "\n",
    "    # Compute absolute difference\n",
    "    diff = torch.abs(original - recon)\n",
    "\n",
    "    # If 3-channel, average over channels to get 2D heatmap\n",
    "    if diff.shape[0] == 3:\n",
    "        diff = diff.mean(dim=0)\n",
    "    elif diff.shape[0] == 1:\n",
    "        diff = diff.squeeze(0)  # [1, H, W] -> [H, W]\n",
    "\n",
    "    # Convert to numpy and show\n",
    "    axes[i].imshow(diff.detach().cpu().numpy(), cmap='hot')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Error Map {i+1}\")\n",
    "\n",
    "plt.suptitle(\"Reconstruction Error Maps\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def to_numpy(img_tensor):\n",
    "    img = img_tensor.detach().cpu()\n",
    "    if img.ndim == 3:\n",
    "        img = img.permute(1, 2, 0)  # [C, H, W] -> [H, W, C]\n",
    "    return img.numpy()\n",
    "\n",
    "def red_pixel_mask(image_tensor, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Create a binary mask where red areas are black, others are white.\n",
    "    Assumes image_tensor shape is [3, H, W], values in [0,1]\n",
    "    \"\"\"\n",
    "    red = image_tensor[0]\n",
    "    green = image_tensor[1]\n",
    "    blue = image_tensor[2]\n",
    "\n",
    "    # Define a simple condition: red > threshold and red > green and blue\n",
    "    mask = (red > threshold) & (red > green) & (red > blue)\n",
    "\n",
    "    # Invert for black-red, white-others\n",
    "    binary_image = torch.ones_like(red)\n",
    "    binary_image[mask] = 0.0  # red pixels -> black\n",
    "    return binary_image\n",
    "\n",
    "# Assume: `images[i]` and `outputs[i]` are in [C, H, W] and in range [0, 1]\n",
    "i = 0  # index of the image to visualize\n",
    "original = images[i]\n",
    "recon = outputs[i]\n",
    "\n",
    "# Error map\n",
    "error = torch.abs(original - recon)\n",
    "error_map = error.mean(dim=0)  # average over channels\n",
    "\n",
    "# Red pixel mask from original image\n",
    "binary_mask = red_pixel_mask(original)\n",
    "\n",
    "# === Display ===\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axes[0].imshow(to_numpy(original))\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(to_numpy(recon))\n",
    "axes[1].set_title(\"Reconstructed Image\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(error_map.detach().cpu().numpy(), cmap='hot')\n",
    "axes[2].set_title(\"Error Map\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(binary_mask.detach().cpu().numpy(), cmap='gray')\n",
    "axes[3].set_title(\"Red Pixels as Black\")\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_reconstruction_accuracy(original, reconstructed):\n",
    "    # original and reconstructed should be tensors of shape [C, H, W], values in [0, 1]\n",
    "    mse = F.mse_loss(reconstructed, original).item()\n",
    "    max_mse = 1.0  # Max possible MSE when values are in [0, 1]\n",
    "    accuracy = 100 * (1 - mse / max_mse)\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "i = 0\n",
    "original = images[i]\n",
    "recon = outputs[i]\n",
    "accuracy = compute_reconstruction_accuracy(original, recon)\n",
    "\n",
    "print(f\"Reconstruction Accuracy: {accuracy:.2f}%\")\n",
    "# Reconstruction accuracy: 99.97%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_embeddings(encoder, dataloader, device):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_filenames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(tqdm(dataloader)):\n",
    "            images = images.to(device)\n",
    "            embeddings = encoder(images).cpu().numpy()\n",
    "\n",
    "            # Extract filenames if using ImageFolder\n",
    "            filenames = [path for path, _ in dataloader.dataset.samples]\n",
    "\n",
    "            print(labels)\n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_filenames.extend(filenames[:len(embeddings)])\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return all_embeddings, all_labels, all_filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embeddings, labels, filenames = extract_embeddings(encoder, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_custom_label(filename):\n",
    "    if filename[0] == 'F':\n",
    "        return 'F'\n",
    "    elif filename[0] == 'G' or filename[:3] == 'Chc':\n",
    "        return 'GC'\n",
    "    elif filename[0] == 'V':\n",
    "        if len(filename) > 1 and filename[1] == 'G':\n",
    "            return 'CC2'\n",
    "        else:\n",
    "            return 'Viv'\n",
    "    return 'Unknown'\n",
    "\n",
    "def extract_embeddings(encoder, dataloader, device):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_filenames = []\n",
    "\n",
    "    # Get filenames from dataset (assuming ImageFolder or similar with `samples`)\n",
    "    full_filenames = [path for path, _ in dataloader.dataset.samples]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        index = 0\n",
    "        for images, _ in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            embeddings = encoder(images).cpu().numpy()\n",
    "            \n",
    "            batch_size = len(embeddings)\n",
    "            batch_filenames = full_filenames[index:index + batch_size]\n",
    "            batch_labels = [get_custom_label(f.split('/')[-1]) for f in batch_filenames]\n",
    "\n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.extend(batch_labels)\n",
    "            all_filenames.extend(batch_filenames)\n",
    "\n",
    "            index += batch_size\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return all_embeddings, all_labels, all_filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "embeddings, labels, filenames = extract_embeddings(encoder, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if 'Unknown' in labels:\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Visualise Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# def visualize_embeddings(embeddings, labels, method='both'):\n",
    "#     embeddings = np.array(embeddings)\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "#     if method in ['pca', 'both']:\n",
    "#         pca = PCA(n_components=2)\n",
    "#         reduced = pca.fit_transform(embeddings)\n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         plt.title(\"PCA Visualization\")\n",
    "#         scatter = plt.scatter(reduced[:, 0], reduced[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "#         plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "#         plt.show()\n",
    "\n",
    "#     if method in ['tsne', 'both']:\n",
    "#         tsne = TSNE(n_components=2, perplexity=30, n_iter=3000, learning_rate='auto', init='pca')\n",
    "#         reduced = tsne.fit_transform(embeddings)\n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         plt.title(\"t-SNE Visualization\")\n",
    "#         scatter = plt.scatter(reduced[:, 0], reduced[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "#         plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Extract embeddings\n",
    "# embeddings, labels, filenames = extract_embeddings(encoder, dataloader, device)\n",
    "\n",
    "# # Visualize\n",
    "# visualize_embeddings(embeddings, labels, method='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# import seaborn as sns\n",
    "\n",
    "# def visualize_and_save_embeddings(embeddings, labels, filenames, output_csv='embeddings.csv', plot_title='Embedding Visualization'):\n",
    "#     # Convert to DataFrame\n",
    "#     df = pd.DataFrame(embeddings)\n",
    "#     df['filename'] = filenames\n",
    "#     df['label'] = labels\n",
    "\n",
    "#     # Save the embeddings with labels and filenames\n",
    "#     df.to_csv(output_csv, index=False)\n",
    "#     print(f\"Embeddings saved to {output_csv}\")\n",
    "\n",
    "#     # Dimensionality reduction\n",
    "#     print(\"Running PCA...\")\n",
    "#     pca = PCA(n_components=50)\n",
    "#     reduced_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "#     print(\"Running t-SNE...\")\n",
    "#     tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, init='pca', random_state=42)\n",
    "#     reduced_tsne = tsne.fit_transform(reduced_pca)\n",
    "\n",
    "#     # Prepare DataFrame for plotting\n",
    "#     plot_df = pd.DataFrame()\n",
    "#     plot_df['x'] = reduced_tsne[:, 0]\n",
    "#     plot_df['y'] = reduced_tsne[:, 1]\n",
    "#     plot_df['label'] = labels\n",
    "\n",
    "#     # Plot using seaborn\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.scatterplot(data=plot_df, x='x', y='y', hue='label', palette='Set2', s=60, alpha=0.7)\n",
    "#     plt.title(plot_title)\n",
    "#     plt.legend(title='Label', loc='best')\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"tsne_visualization.png\")\n",
    "#     plt.show()\n",
    "#     print(\"Visualization saved as tsne_visualization.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# visualize_and_save_embeddings(embeddings, labels, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import seaborn as sns\n",
    "\n",
    "# def plot_2d(data, labels, title, filename):\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=labels, palette='Set2', s=60, alpha=0.7)\n",
    "#     plt.title(title)\n",
    "#     plt.legend(title='Label', loc='best')\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(filename)\n",
    "#     plt.close()\n",
    "#     print(f\"Saved 2D plot: {filename}\")\n",
    "\n",
    "# def plot_3d(data, labels, title, filename):\n",
    "#     fig = plt.figure(figsize=(12, 10))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "#     unique_labels = list(set(labels))\n",
    "#     colors = plt.cm.get_cmap('Set2', len(unique_labels))\n",
    "\n",
    "#     for i, label in enumerate(unique_labels):\n",
    "#         idx = [j for j, l in enumerate(labels) if l == label]\n",
    "#         ax.scatter(data[idx, 0], data[idx, 1], data[idx, 2], label=label, color=colors(i), s=60, alpha=0.7)\n",
    "\n",
    "#     ax.set_title(title)\n",
    "#     ax.legend(title='Label', loc='upper right')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(filename)\n",
    "#     plt.close()\n",
    "#     print(f\"Saved 3D plot: {filename}\")\n",
    "\n",
    "# def visualize_and_save_embeddings(embeddings, labels, filenames, output_csv='embeddings.csv'):\n",
    "#     # Save to CSV\n",
    "#     df = pd.DataFrame(embeddings)\n",
    "#     df['filename'] = filenames\n",
    "#     df['label'] = labels\n",
    "#     df.to_csv(output_csv, index=False)\n",
    "#     print(f\"Saved embeddings to {output_csv}\")\n",
    "\n",
    "#     # PCA 2D and 3D\n",
    "#     print(\"Running PCA...\")\n",
    "#     pca_2d = PCA(n_components=2).fit_transform(embeddings)\n",
    "#     pca_3d = PCA(n_components=3).fit_transform(embeddings)\n",
    "#     plot_2d(pca_2d, labels, \"PCA 2D Visualization\", \"pca_2d.png\")\n",
    "#     plot_3d(pca_3d, labels, \"PCA 3D Visualization\", \"pca_3d.png\")\n",
    "\n",
    "#     # t-SNE 2D and 3D\n",
    "#     print(\"Running t-SNE...\")\n",
    "#     pca_50 = PCA(n_components=50).fit_transform(embeddings)  # Use PCA for faster t-SNE\n",
    "#     tsne_2d = TSNE(n_components=2, perplexity=30, learning_rate=200, init='pca', random_state=42).fit_transform(pca_50)\n",
    "#     tsne_3d = TSNE(n_components=3, perplexity=30, learning_rate=200, init='pca', random_state=42).fit_transform(pca_50)\n",
    "#     plot_2d(tsne_2d, labels, \"t-SNE 2D Visualization\", \"tsne_2d.png\")\n",
    "#     plot_3d(tsne_3d, labels, \"t-SNE 3D Visualization\", \"tsne_3d.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# visualize_and_save_embeddings(embeddings, labels, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Save embedding csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Extract Embeddings ===\n",
    "filenames = [path[0] for path in dataset.samples]\n",
    "embeddings = []\n",
    "names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, _) in enumerate(dataloader):  # images: [4, 1, H, W]\n",
    "        images = images.to(device)\n",
    "\n",
    "        ft = torch.fft.fft2(images)\n",
    "        ft_mag = torch.abs(ft)\n",
    "\n",
    "        z_img = model.image_encoder(images)      # [4, 512]\n",
    "        z_ft = model.fourier_encoder(ft_mag)     # [4, 512]\n",
    "        z = torch.cat([z_img, z_ft], dim=1)      # [4, 1024]\n",
    "\n",
    "        z_np = z.cpu().numpy()                   # (4, 1024)\n",
    "\n",
    "        # Save each embedding and its filename\n",
    "        for j in range(z_np.shape[0]):\n",
    "            embeddings.append(z_np[j])\n",
    "            names.append(dataset.samples[i * dataloader.batch_size + j][0])\n",
    "\n",
    "\n",
    "# === Save to DataFrame ===\n",
    "df = pd.DataFrame(embeddings)\n",
    "df.insert(0, \"filename\", names)\n",
    "df.to_csv(\"fft_image_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(embeddings, columns=[f'feature_{i}' for i in range(embeddings.shape[1])])\n",
    "df['filename'] = [os.path.basename(f) for f in filenames]\n",
    "# label = []\n",
    "# files = list(df['filename'])\n",
    "# for f in files:\n",
    "#     if f[0] == 'F':\n",
    "#         label.append('F')\n",
    "#     elif f[0] == 'G' or f[:3] == 'Chc':\n",
    "#         label.append('GC')\n",
    "#     elif f[0] == 'V':\n",
    "#         if f[1] == 'G':\n",
    "#             label.append('CC2')\n",
    "#         else:\n",
    "#             label.append('Viv')\n",
    "df['label'] = labels#['A','A','B','C','D','C']\n",
    "df.to_csv('CH2_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "list(df.columns)[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# === Step 1: Load data ===\n",
    "df = pd.read_csv(\"encoder_embeddings.csv\")  # replace with your CSV file path\n",
    "df.drop('filename',axis=1,inplace=True)\n",
    "features = df.columns[:-1]  # assuming last column is 'label'\n",
    "X = df[features].values\n",
    "y = df['label'].values  # labels: 'A', 'B', 'C', 'D'\n",
    "\n",
    "# === Step 2: Standardize features ===\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# === Step 3: PCA ===\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_std)\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_std)\n",
    "\n",
    "# === Step 4: Visualization ===\n",
    "colors = {'F': 'red', 'GC': 'blue', 'Viv': 'green', 'CC2': 'orange'}\n",
    "\n",
    "# -- 2D Plot --\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in colors:\n",
    "    idx = y == label\n",
    "    plt.scatter(X_pca_2d[idx, 0], X_pca_2d[idx, 1], label=label, alpha=0.6, c=colors[label])\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('2D PCA')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -- 3D Plot --\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for label in colors:\n",
    "    idx = y == label\n",
    "    ax.scatter(X_pca_3d[idx, 0], X_pca_3d[idx, 1], X_pca_3d[idx, 2], label=label, alpha=0.6, color=colors[label])\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('3D PCA')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising all embeddings of CH1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# === Step 1: Load Data ===\n",
    "df = pd.read_csv(\"CH1_embeddings.csv\")  # Replace with your actual file path\n",
    "df.drop('filename',axis=1,inplace=True)\n",
    "features = df.columns[:-1]  # Assuming the last column is 'label'\n",
    "X = df[features].values\n",
    "y = df['label'].values  # Labels: 'A', 'B', 'C', 'D'\n",
    "\n",
    "# === Step 2: Standardize Features ===\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# === Step 3: PCA ===\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_std)\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_std)\n",
    "\n",
    "# === Step 4: t-SNE ===\n",
    "tsne_2d = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne_2d = tsne_2d.fit_transform(X_std)\n",
    "\n",
    "tsne_3d = TSNE(n_components=3, perplexity=30, random_state=42)\n",
    "X_tsne_3d = tsne_3d.fit_transform(X_std)\n",
    "\n",
    "# === Step 5: Plotting ===\n",
    "colors = {'F': 'red', 'GC': 'blue', 'Viv': 'green', 'CC2': 'orange'}\n",
    "\n",
    "def plot_2d(data, title, labels, color_map):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for label in color_map:\n",
    "        idx = labels == label\n",
    "        plt.scatter(data[idx, 0], data[idx, 1], c=color_map[label], label=label, alpha=0.6)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_3d(data, title, labels, color_map):\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for label in color_map:\n",
    "        idx = labels == label\n",
    "        ax.scatter(data[idx, 0], data[idx, 1], data[idx, 2], label=label, alpha=0.6, color=color_map[label])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_zlabel('Component 3')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting all\n",
    "plot_2d(X_pca_2d, 'PCA 2D Plot', y, colors)\n",
    "plot_3d(X_pca_3d, 'PCA 3D Plot', y, colors)\n",
    "plot_2d(X_tsne_2d, 't-SNE 2D Plot', y, colors)\n",
    "plot_3d(X_tsne_3d, 't-SNE 3D Plot', y, colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sample: replace this with your own features and labels\n",
    "# features: (num_samples, num_features)\n",
    "# labels: list or array of class labels (optional, for color)\n",
    "\n",
    "# === Step 1: Load Data ===\n",
    "df = pd.read_csv(\"CH1_embeddings.csv\")  # Replace with your actual file path\n",
    "df.drop('filename',axis=1,inplace=True)\n",
    "features = df.columns[:-1]  # Assuming the last column is 'label'\n",
    "X = df[features].values\n",
    "labels = df['label'].values  # Labels: 'A', 'B', 'C', 'D'\n",
    "\n",
    "# === Step 2: Standardize Features ===\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# PCA reduction to 3D\n",
    "pca_3_components = PCA(n_components=3)\n",
    "pca_result = pca_3_components.fit_transform(X_std)\n",
    "\n",
    "# Create a DataFrame for plotly\n",
    "newdf = pd.DataFrame()\n",
    "newdf['PCA1'] = pca_result[:, 0]\n",
    "newdf['PCA2'] = pca_result[:, 1]\n",
    "newdf['PCA3'] = pca_result[:, 2]\n",
    "newdf['Label'] = labels\n",
    "\n",
    "# Interactive 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    newdf, x='PCA1', y='PCA2', z='PCA3',\n",
    "    color='Label',\n",
    "    title='3D PCA Visualization',\n",
    "    labels={'Label': 'Class'},\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-Layer CNN Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('data3')\n",
    "os.mkdir('data4')\n",
    "os.mkdir('data5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:29:53.830884Z",
     "iopub.status.busy": "2025-05-09T16:29:53.830051Z",
     "iopub.status.idle": "2025-05-09T16:29:54.215553Z",
     "shell.execute_reply": "2025-05-09T16:29:54.214955Z",
     "shell.execute_reply.started": "2025-05-09T16:29:53.830861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-09T16:32:11.488Z",
     "iopub.execute_input": "2025-05-09T16:29:58.498967Z",
     "iopub.status.busy": "2025-05-09T16:29:58.497883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==== 1. CNN Model ====\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3),    # Layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3),   # Layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3),  # Layer 3\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),  # Layer 4\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        return x\n",
    "\n",
    "# ==== 2. Image Transform & Dataset ====\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1440, 1920)),\n",
    "    transforms.ToTensor(),  # Converts to [C, H, W] in range [0, 1]\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(\"/home/gojian/Notebooks/GeneLab/AI-ML_AWG/Working_Code/data_folder_images/data2\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# ==== 3. Extract Embeddings ====\n",
    "model = SimpleCNN()\n",
    "model.eval()\n",
    "\n",
    "embeddings = []\n",
    "filenames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, _ in tqdm(dataloader):\n",
    "        emb = model(img)  # Shape: [1, N]\n",
    "        embeddings.append(emb.squeeze().numpy())\n",
    "        \n",
    "        path, _ = dataset.samples[len(filenames)]\n",
    "        filename = os.path.basename(path)\n",
    "        filenames.append(filename)\n",
    "\n",
    "# ==== 4. Save to CSV ====\n",
    "df = pd.DataFrame(embeddings)\n",
    "df.insert(0, \"filename\", filenames)\n",
    "df.to_csv(\"simpleCNN_embeddings_CH1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
